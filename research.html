<!DOCTYPE html>
<html>

<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-167680016-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-167680016-1');
</script>

    <link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">
    <link type="text/css" href="style.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Caslon+Text&display=swap" rel="stylesheet">


    <title>
        Kathleen A. Creel
    </title>
</head>

<body>
      <div id="title">
        <a href="index.html">
            <center>
                <h1>Kathleen A. Creel</h1>
            </center>
        </a>
    </div>
    <div id="menu">

         <a href="index.html">
            <div class="menu-links" id="home">
                <p>About</p>
            </div>
        </a>
        
        <a href="research.html">
            <div class="menu-links" id="research">
                <p>Research</p>
            </div>
        </a>
        <a href="teaching.html">
            <div class="menu-links" id="teaching">
                <p>Teaching</p>
            </div>
        </a>
        <a href="contact.html">
            <div class="menu-links" id="contact">
                <p>Contact</p>
            </div>
        </a>
        <a href="https://www.dropbox.com/s/11lnywn0aq668jh/Creel_CV_2020.pdf?dl=0">
        <!-- <a href="https://share.getcloudapp.com/z8unlQGD"> -->
            <div class="menu-links" id="cv">
                <p>CV</p>
            </div>
        </a>
    </div>
    <!-- Above is the end of the menu bar div and all the links inside. 
 -->

 <section class="section1">


<div class="green">
   <table> 

    <tr>

    <td class="course-name">Research Interests</td>
    <td class="course-description">

        <p>
        My work broadly concerns philosophy of machine learning, ethics of AI, and general philosophy of science.  I am interested in how humans can best use computation to understand themselves and their world.  How can we gain scientific understanding with opaque, black-box computational methods? What ways of explaining machine learning best serve scientific and public life?  How should supposedly autonomously generated concepts inspire us to revise our own?  In answering these sorts of questions, I connect traditional topics in philosophy of science such as explanation, reference, and natural kinds with a practice-based approach to the study of methods in contemporary machine learning. By examining these epistemic and normative questions, I outline more fruitful uses of machine learning for human flourishing.

        <!-- My research explores the moral and political philosophy of uses of machine learning in public life, and the epistemology of machine learning in science. In current uses of machine learning, powerful mathematical tools promise to expand human cognitive capacities, but threaten to endanger epistemic and moral goods while doing so. I formulate theories of transparency and explanation in machine learning and use them to address ethical, political, and scientific problems in machine learning’s current applications. -->
  

        </p>
    </td>
    <td class="research-pics">

        

    </td>
</tr>
</table>
</div>
     <table>
        
        <tr>
            <td class="course-name"><div >Philosophy of <br> Machine Learning</div></td>
            <td class="course-description"><div>
        <p> Deep neural networks are often thought to be opaque black boxes. However, the sense in which they are opaque is philosophically interesting since every component of the system can be individually surveyed.  What would it mean for such a sytem to be transparent? Is transparency required for trust? I explore explanatory strategies for opaque machine learning both as it is used for science and as it is used in public life.  For an example of this work see my paper on <a href="#transparency">transparency</a> below.
            <br>
        I am also interested in how machine learning influences scientific and social categories. When new features crosscut our existing scientific or social categories, how do we or should we revise our understanding of the world and its contents? 
        </p>
    </div></td>

     <td class="research-pics">
<div id="neural-net" class="research-circles"></div>
     </td>
  </tr>

  <tr> 
            <td class="course-name"><div >Ethics of <br> Artificial Intelligence</div></td>
            <td class="course-description"><div>
        <p> Answers to epistemic questions about transparency and explanation are relevant to the use of algorithmic decision making in public life. I consider case studies of non-state use of automated decision-making, such as automated hiring systems and loan approval algorithms. What implications do adversarial examples and other known features of deep learning systems have for transparency and fairness? <a href="#leviathan">A paper from this project </a> has been accepted at <b style="color:rgb(88, 88, 88)"><a href='https://facctconference.org/2021/acceptedpapers.html'>ACM FAccT 2021.</a></b>
        <br> 

        Whether or not these systems are black boxes, if they are treated as such we may come to trust them on a testimonial basis.  I explore questions of machine testimony and of appropriate trust in automated decisionmaking systems.
<!-- 
Drawing on contemporary pragmatists like Elizabeth Anderson, I propose an explanatory framework for algorithmic decisions made in public life that respects the multiplicity of explanatory purposes.  Although an applicant may first be interested in the reasons for a loan's denial, upon learning more about the system she might come to be interested in whether protected features such as race were used in the making the decision and whether the decision-making system as a whole is unjust.  A pragmatist account of meaning for purpose allows us to provide reasons that shift with the epistemic goals of the users. --> 

        </p>
    </div></td>
     <td class="research-pics">
<div id="ai-ethics-pic" class="research-circles"></div>
     </td>
  </tr>

  <tr>
            <td class="course-name"><div >General Philosophy of Science</div></td>
            <td class="course-description"><div>
        <p> Machine learning is only one species of a genus of scientific methods for finding patterns in data.  This pattern-finding capacity is often thought to support the discovery of scientific phenomena, or the recognition of patterns that reflect activity and causal processes in the world rather than noise or instrument-caused artifacts of the data.  In my work in general philosophy of science, I investigate the distinguishment of signal from noise and phenomena from artifact. 

I am also interested in the normativity of scientific beliefs.  Arguments for popular forms of scientific explanation such as mechanistic explanation implicitly rely on normative theories of epistemic reason-giving.  I am interested in borrowing tools from metaethics to examine the nature/normativity of scientific belief formation.  
 </p>
    </div></td>

     <td class="research-pics">
<div id="parrot" class="research-circles"></div>
     </td>
  </tr>

  

  <tr>
            <td class="course-name"><div >History of Philosophy</div></td>
            <td class="course-description" style="border-bottom:dotted 1px grey;"><div>
        <p> Google’s Ali Rahimi has called machine learning a "new alchemy": a pre-paradigmatic science whose notable successes outstrip the scientific theory meant to explain them.  Early modern "natural philosophers" like Bacon, Boyle, and du Châtelet faced a similar gap between their practical ability to predict or control and their capacity to explain those successes with existing scientific theories.  In this gap flowered an integrated pursuit of observation, experimentation, epistemology, and metaphysics.  Lessons from this period, especially the methodological pursuits of Scottish enlightenment scientists such as Joseph Black and James Hutton, inform my work. <br>

        Likewise, machine learning holds out the promise, or perhaps illusion, that our technology-enhanced capacities can outstrip the human -- that we can get outside ourselves.   My research considers how to make automated decision-making systems more fair and just while grounding them in a naturalistic understanding of human sympathy and social relationships.  This work focuses on early modern sentimentalists such as David Hume, Adam Smith, and Sophie de Grouchy.  For example, Hume's theory of justice and the caprice of power underlies my most recent manuscript: the Algorithmic Leviathan, concerning arbitrariness by automated decision-making systems.
         </p>
    </div></td>
     <td class="research-pics">
        <div id="earlymodern" class="research-circles"></div>
     </td>
  </tr>
</table>


<table>

<tr>
    <td class="course-name">Papers</td>


    <tr>
    <td class="course-name"></td>

    <td class="course-description"><p id="outcomehomogenization"> 
        <h3>Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes with Connor Toups, Rishi Bommasani, Sarah Bana, Dan Jurafsky, and Percy Liang.<br><i>NeurIPS (2023) </i></h3>   

        Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we find these improvements rarely reduce the prevalence of systemic failure. Instead, the benefits of these improvements predominantly accrue to individuals who are already correctly classified by other models. In light of these trends, we consider medical imaging for dermatology where the costs of systemic failure are especially high. While traditional analyses reveal racial performance disparities for both models and humans, ecosystem-level analysis reveals new forms of racial disparity in model predictions that do not present in human predictions. These examples demonstrate ecosystem-level analysis has unique strengths for characterizing the societal impact of machine learning.
        </p>

        <ul>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://arxiv.org/abs/2307.05862'>Link to paper</a>  </li>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://hai.stanford.edu/news/when-ai-systems-systemically-fail'>Link to blog post </a>  </li>
        </ul> 

    </td>

    <td class="research-pics">
        <div id="neurips-pic" class="research-circles"></div>

    </td>

</tr>
 
    <td class="course-description"><p id="patientvalues"> 
        <h3>Clinical decisions using AI must consider patient values, with Jonathan Birch, Abhinav Jha, and Anya Plutynski <br><i>Nature Medicine (2022) </i></h3>   

            Abstract: Built-in decision thresholds for AI diagnostics are ethically problematic, as patients may differ in their attitudes about the risk of false-positive and false-negative results, which will require that clinicians assess patient values.
        </p>

        <ul>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='rdcu.be/cF5Cq'>Link to read paper</a>  </li>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://dx.doi.org/10.1038/s41591-021-01624-y'>Link to paper (DOI)</a>  </li>
        </ul> 

    </td>

    <td class="research-pics"> 
     <div id="nature-med" class="research-circles"></div> 
    </td> 
</tr>

    

<tr>
    <td class="course-name"></td>

    <td class="course-description"><p id="outcomehomogenization"> 
        <h3>Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization? with Rishi Bommasani, Ananya Kumar, Dan Jurafsky, and Percy Liang.<br><i>NeurIPS (2022) </i></h3>   

        Abstract: As the scope of machine learning broadens, we observe a recurring theme of algorithmic monoculture: the same systems, or systems that share components (e.g. training data), are deployed by multiple decision-makers. While sharing offers clear advantages (e.g. amortizing costs), does it bear risks? 

        We introduce and formalize one such risk, outcome homogenization: the extent to which particular individuals or groups experience negative outcomes from all decision-makers. If the same individuals or groups exclusively experience undesirable outcomes, this may institutionalize systemic exclusion and reinscribe social hierarchy. 

        To relate algorithmic monoculture and outcome homogenization, we propose the component-sharing hypothesis: if decision-makers share components like training data or specific models, then they will produce more homogeneous outcomes. We test this hypothesis on algorithmic fairness benchmarks, demonstrating that sharing training data reliably exacerbates homogenization, with individual-level effects generally exceeding group-level effects. 

        Further, given the dominant paradigm in AI of foundation models, i.e. models that can be adapted for myriad downstream tasks, we test whether model sharing homogenizes outcomes across tasks. We observe mixed results: we find that for both vision and language settings, the specific methods for adapting a foundation model significantly influence the degree of outcome homogenization. We conclude with philosophical analyses of and societal challenges for outcome homogenization, with an eye towards implications for deployed machine learning systems.
        
        </p>

        <ul>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://arxiv.org/abs/2211.13972'>Link to paper</a>  </li>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://hai.stanford.edu/events/hai-weekly-seminar-kathleen-creel-0'>Link to video presentation </a>  </li>
        </ul> 

    </td>

    <td class="research-pics">
        <div id="neurips-pic" class="research-circles"></div>

    </td>

</tr>



<tr>
    <td class="course-name"></td>

    <td class="course-description"><p id="artificialknowing"> 
        <h3>Artificial Knowing Otherwise, with Os Keyes.<br><i>Feminist Philosophy Quarterly (2022) </i></h3>   

        Abstract: While feminist critiques of AI are increasingly common in the scholarly literature, they are by no means new. Alison Adam’s Artificial Knowing (1998) brought a feminist social and epistemological stance to the analysis of AI, critiquing the symbolic AI systems of her day and proposing constructive alternatives. In this paper, we seek to revisit and renew Adam’s arguments and methodology, exploring their resonances with current feminist concerns and their relevance to contemporary machine learning. Like Adam, we ask how new AI methods could be adapted for feminist purposes and what role new technologies might play in addressing concerns raised by feminist epistemologists and theorists about algorithmic systems. In particular, we highlight distributed and federated learning as providing partial solutions to the power-oriented concerns that have stymied efforts to make machine learning systems more representative and pluralist.
        
        </p>

        <ul>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://ojs.lib.uwo.ca/index.php/fpq/article/view/14313'>Link to paper</a>  </li>
        </ul> 

    </td>

    <td class="research-pics">
        <div id="fpq-pic" class="research-circles"></div>

    </td>

</tr>



    

<tr>
    <td class="course-name"></td>

    <td class="course-description"><p><p id="leviathan"> 
        <h3>The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making, with Deborah Hellman <br><i>ACM FAccT 2021, Canadian Journal of Philosophy, 2022 </i></h3> 

        Abstract: Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern?

        We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are "fair" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.
        </p>

        <ul>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3786377'>Link to preprint</a>  </li>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/algorithmic-leviathan-arbitrariness-fairness-and-opportunity-in-algorithmic-decisionmaking-systems/3AA0ECA77F8622488E9DB0834287215B'>Link to paper (open access) </a>  </li>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://hai.stanford.edu/events/hai-weekly-seminar-kathleen-creel'>Link to video presentation </a>  </li>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://www.techsequences.org/podcasts/2021/04/machine-made-decisions-consequences-of-consistency/'>Link to podcast discussion </a>  </li>
        </ul>
    </td>

    <td class="research-pics">
        <div id="cjp-pic" class="research-circles"></div>

    </td>
 </tr>


 <tr>

    <td class="course-name"></td>

    <td class="course-description"><p id="datacollection"> 
        <h3>Privacy and Paternalism: The Ethics of Student Data Collection, with Tara Dixit <br><i>MIT Case Studies in Social and Ethical Responsibilities of Computing (2022) </i></h3>   

        Abstract: 
        </p>

        <ul>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://doi.org/10.21428/2c646de5.b725319a'>Link to paper (open access)</a>  </li>
        </ul> 
    </td>

    <td class="research-pics">
     <div id="case-studies-pic" class="research-circles"></div>

    </td>

</tr>


<tr>
    <td class="course-name"></td>

    <td class="course-description"><p id="foundationmodels"> 
    <h3>On the Opportunities and Risks of Foundation Models, with Rishi Bommasani et. al., see full author list at link. at <i>arXiv (2021) </i></h3>   

    Abstract: AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.
    </p>

    <ul>
        <li> <b style="color:rgb(88, 88, 88)"> <a href='https://arxiv.org/abs/2108.07258'>Link to arXiv</a></li>
        <li> <b style="color:rgb(88, 88, 88)"> <a href='https://youtu.be/T2e6Y37EAGo?t=3071'>Link to video presentation</a></li>
    </ul> 

    </td>

    <td class="research-pics">
        <div id="arxiv-pic" class="research-circles"></div>

    </td>

 </tr> 

<tr>
    <td class="course-name"></td>
    <td class="course-description"><p id="transparency"> 
        <h3>Transparency in Complex Computational Systems <br><i>Philosophy of Science (October, 2020, Volume 87 Issue 4) </i>
        <br> <i> Winner of the <b style="color:rgb(88, 88, 88)"> <a href='https://philsci.org/ernest_nagel_early-career_scho.php/'>Ernest Nagel Early-Career Scholar Essay Award</a> </i>
        </h3>   

        Abstract: Scientists depend on complex computational systems that are often ineliminably opaque, to the detriment of our ability to give scientific explanations and detect artifacts. Some philosophers have suggested treating opaque systems instrumentally, but computer scientists developing strategies for increasing transparency are correct in finding this unsatisfying. Instead, I propose an analysis of transparency as having three forms: transparency of the algorithm, the realization of the algorithm in code, and the way that code is run on particular hardware and data. This targets the transparency most useful for a task, avoiding instrumentalism by providing partial transparency when full transparency is impossible.
        </p>

        <ul>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='http://philsci-archive.pitt.edu/16669/'>Link to preprint</a>  </li>
            <li> <b style="color:rgb(88, 88, 88)"> <a href='https://www.journals.uchicago.edu/doi/pdfplus/10.1086/709729/'>Link to paper</a>  </li>
        </ul>
    </td>

 <td class="research-pics">
     <div id="phil-sci-pic" class="research-circles"></div>

 </td>
</tr>

</table>  
 
</section>   
    
</body>

</html>