<!DOCTYPE html>
<html>

<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-167680016-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-167680016-1');
</script>

    <link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">
    <link type="text/css" href="style.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Caslon+Text&display=swap" rel="stylesheet">


    <title>
        Kathleen A. Creel
    </title>
</head>

<body>
      <div id="title">
        <a href="index.html">
            <center>
                <h1>Kathleen A. Creel</h1>
            </center>
        </a>
    </div>
    <div id="menu">

         <a href="index.html">
            <div class="menu-links" id="home">
                <p>About</p>
            </div>
        </a>
        
        <a href="research.html">
            <div class="menu-links" id="research">
                <p>Research</p>
            </div>
        </a>
        <a href="teaching.html">
            <div class="menu-links" id="teaching">
                <p>Teaching</p>
            </div>
        </a>
        <a href="contact.html">
            <div class="menu-links" id="contact">
                <p>Contact</p>
            </div>
        </a>
        <a href="https://www.dropbox.com/s/11lnywn0aq668jh/Creel_CV_2020.pdf?dl=0">
        <!-- <a href="https://share.getcloudapp.com/z8unlQGD"> -->
            <div class="menu-links" id="cv">
                <p>CV</p>
            </div>
        </a>
    </div>
    <!-- Above is the end of the menu bar div and all the links inside. 
 -->

 <section class="section1">


<div class="green">
   <table> 

    <tr>

    <td class="course-name">Research Interests</td>
    <td class="course-description">

        <p>
        My work broadly concerns philosophy of machine learning, ethics of AI, and general philosophy of science.  I am interested in how humans can best use computation to understand themselves and their world.  How can we gain scientific understanding with opaque, black-box computational methods? What ways of explaining machine learning best serve scientific and public life?  How should supposedly autonomously generated concepts inspire us to revise our own?  In answering these sorts of questions, I connect traditional topics in philosophy of science such as explanation, reference, and natural kinds with a practice-based approach to the study of methods in contemporary machine learning. By examining these epistemic and normative questions, I outline more fruitful uses of machine learning for human flourishing.

        <!-- My research explores the moral and political philosophy of uses of machine learning in public life, and the epistemology of machine learning in science. In current uses of machine learning, powerful mathematical tools promise to expand human cognitive capacities, but threaten to endanger epistemic and moral goods while doing so. I formulate theories of transparency and explanation in machine learning and use them to address ethical, political, and scientific problems in machine learning’s current applications. -->
  

        </p>
    </td>
    <td class="research-pics">

        

    </td>
</tr>
</table>
</div>
     <table>
        
        <tr>
            <td class="course-name"><div >Philosophy of <br> Machine Learning</div></td>
            <td class="course-description"><div>
        <p> Deep neural networks are often thought to be opaque black boxes. However, the sense in which they are opaque is philosophically interesting since every component of the system can be individually surveyed.  What would it mean for such a sytem to be transparent? Is transparency required for trust? I explore explanatory strategies for opaque machine learning both as it is used for science and as it is used in public life.  For an example of this work see my paper on <a href="#transparency">transparency</a> below.
            <br>
        I am also interested in how machine learning influences scientific and social categories. When new features crosscut our existing scientific or social categories, how do we or should we revise our understanding of the world and its contents? 
        </p>
    </div></td>

     <td class="research-pics">
<div id="neural-net" class="research-circles"></div>
     </td>
  </tr>

  <tr> 
            <td class="course-name"><div >Ethics of <br> Artificial Intelligence</div></td>
            <td class="course-description"><div>
        <p> Answers to epistemic questions about transparency and explanation are relevant to the use of algorithmic decision making in public life. I consider case studies of non-state use of automated decision-making, such as automated hiring systems and loan approval algorithms. What implications do adversarial examples and other known features of deep learning systems have for transparency and fairness? <a href="#leviathan">A paper from this project </a> has been accepted at <b style="color:rgb(88, 88, 88)"><a href='https://facctconference.org/2021/acceptedpapers.html'>ACM FAccT 2021.</a></b>
        <br> 

        Whether or not these systems are black boxes, if they are treated as such we may come to trust them on a testimonial basis.  I explore questions of machine testimony and of appropriate trust in automated decisionmaking systems.
<!-- 
Drawing on contemporary pragmatists like Elizabeth Anderson, I propose an explanatory framework for algorithmic decisions made in public life that respects the multiplicity of explanatory purposes.  Although an applicant may first be interested in the reasons for a loan's denial, upon learning more about the system she might come to be interested in whether protected features such as race were used in the making the decision and whether the decision-making system as a whole is unjust.  A pragmatist account of meaning for purpose allows us to provide reasons that shift with the epistemic goals of the users. --> 

        </p>
    </div></td>
     <td class="research-pics">
<div id="ai-ethics-pic" class="research-circles"></div>
     </td>
  </tr>

  <tr>
            <td class="course-name"><div >General Philosophy of Science</div></td>
            <td class="course-description"><div>
        <p> Machine learning is only one species of a genus of scientific methods for finding patterns in data.  This pattern-finding capacity is often thought to support the discovery of scientific phenomena, or the recognition of patterns that reflect activity and causal processes in the world rather than noise or instrument-caused artifacts of the data.  In my work in general philosophy of science, I investigate the distinguishment of signal from noise and phenomena from artifact. 

I am also interested in the normativity of scientific beliefs.  Arguments for popular forms of scientific explanation such as mechanistic explanation implicitly rely on normative theories of epistemic reason-giving.  I am interested in borrowing tools from metaethics to examine the nature/normativity of scientific belief formation.  
 </p>
    </div></td>

     <td class="research-pics">
<div id="parrot" class="research-circles"></div>
     </td>
  </tr>

  

  <tr>
            <td class="course-name"><div >History of Philosophy</div></td>
            <td class="course-description" style="border-bottom:dotted 1px grey;"><div>
        <p> Google’s Ali Rahimi has called machine learning a "new alchemy": a pre-paradigmatic science whose notable successes outstrip the scientific theory meant to explain them.  Early modern "natural philosophers" like Bacon, Boyle, and du Châtelet faced a similar gap between their practical ability to predict or control and their capacity to explain those successes with existing scientific theories.  In this gap flowered an integrated pursuit of observation, experimentation, epistemology, and metaphysics.  Lessons from this period, especially the methodological pursuits of Scottish enlightenment scientists such as Joseph Black and James Hutton, inform my work. <br>

        Likewise, machine learning holds out the promise, or perhaps illusion, that our technology-enhanced capacities can outstrip the human -- that we can get outside ourselves.   My research considers how to make automated decision-making systems more fair and just while grounding them in a naturalistic understanding of human sympathy and social relationships.  This work focuses on early modern sentimentalists such as David Hume, Adam Smith, and Sophie de Grouchy.  For example, Hume's theory of justice and the caprice of power underlies my most recent manuscript: the Algorithmic Leviathan, concerning arbitrariness by automated decision-making systems.
         </p>
    </div></td>
     <td class="research-pics">
<div id="earlymodern" class="research-circles"></div>
     </td>
  </tr>

    </table>


    <table>

        <tr><td class="course-name">Papers</td>

        <td class="course-description"><p id="transparency"> 
            <h3>Transparency in Complex Computational Systems <br><i>Philosophy of Science (October, 2020, Volume 87 Issue 4) </i></h3>   

Abstract: Scientists depend on complex computational systems that are often ineliminably opaque, to the detriment of our ability to give scientific explanations and detect artifacts. Some philosophers have suggested treating opaque systems instrumentally, but computer scientists developing strategies for increasing transparency are correct in finding this unsatisfying. Instead, I propose an analysis of transparency as having three forms: transparency of the algorithm, the realization of the algorithm in code, and the way that code is run on particular hardware and data. This targets the transparency most useful for a task, avoiding instrumentalism by providing partial transparency when full transparency is impossible.
        </p>

            <ul>
<li> <b style="color:rgb(88, 88, 88)"> <a href='https://www.journals.uchicago.edu/doi/pdfplus/10.1086/709729/'>Link to paper</a>  </li>
    <li> <b style="color:rgb(88, 88, 88)"> <a href='http://philsci-archive.pitt.edu/16669/'>Link to preprint</a>  </li>
</ul>
</td>

 <td class="research-pics">
<!-- <div id="transparency-pic" class="research-circles"></div> -->
 </td>
</tr>

<tr><td class="course-name"></td>

    <td class="course-description"><p><p id="leviathan"> 
            <h3>The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making, with Deborah Hellman <br><i>ACM FAccT 2021, Canadian Journal of Philosophy, forthcoming </i></h3> 
Abstract: Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern?

We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are "fair" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.
        </p>

    <ul>
<li> <b style="color:rgb(88, 88, 88)"> <a href='https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3786377'>Link to preprint</a>  </li>
    <li> <b style="color:rgb(88, 88, 88)"> <a href='https://dl.acm.org/doi/10.1145/3442188.3445942'>Link to FAccT abstract </a>  </li>
        <li> <b style="color:rgb(88, 88, 88)"> <a href='https://hai.stanford.edu/events/hai-weekly-seminar-kathleen-creel'>Link to video presentation </a>  </li>
        <li> <b style="color:rgb(88, 88, 88)"> <a href='https://www.techsequences.org/podcasts/2021/04/machine-made-decisions-consequences-of-consistency/'>Link to podcast discussion </a>  </li>


            
</ul>
</td>

 <td class="research-pics">
<!-- <div id="transparency-pic" class="research-circles"></div> -->
 </td>
 </tr>


<tr><td class="course-name"></td>

 <td class="course-description"><p id="patientvalues"> 
    <h3>On the Opportunities and Risks of Foundation Models, with Rishi Bommasani* et. al., see full author list at link. at <br><i>arXiv (2021) </i></h3>   

    ``,'' \textit{arXiv} (2021) Cowritten with Rishi Bommasani* et. al., see full author list at \href{}{https://arxiv.org/abs/2108.07258}

Abstract: AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.
        </p>

<ul>
<li> <b style="color:rgb(88, 88, 88)"> <a href='https://arxiv.org/abs/2108.07258'>arXiv link</a>  </li>
</ul> 


        </td>

<td class="research-pics">
<!-- <div id="transparency-pic" class="research-circles"></div> -->
 </td>



    </tr>


  <tr><td class="course-name"></td>

 <td class="course-description"><p id="patientvalues"> 
    <h3>Clinical decisions using AI must consider patient values, with Jonathan Birch, Abhinav Jha, and Anya Plutynski <br><i>Nature Medicine (2022) </i></h3>   

Abstract: Built-in decision thresholds for AI diagnostics are ethically problematic, as patients may differ in their attitudes about the risk of false-positive and false-negative results, which will require that clinicians assess patient values.
        </p>

<ul>
<li> <b style="color:rgb(88, 88, 88)"> <a href='rdcu.be/cF5Cq'>Link to read paper</a>  </li>
    <li> <b style="color:rgb(88, 88, 88)"> <a href='https://dx.doi.org/10.1038/s41591-021-01624-y'>DOI</a>  </li>
</ul> 


        </td>

<td class="research-pics">
<!-- <div id="transparency-pic" class="research-circles"></div> -->
 </td>



    </tr>




  <!--   <tr><td class="course-name">Under Review</td>

        <td class="course-description"> <h3>[Paper on signal and noise] <br> <i>R&R at BJPS</i></h3> 


            <ul>
    <li>Draft available on request.</li>
</ul>


        </td>

    </tr>
 -->


</table>


    
 
 </section>


   

    
    
</body>

</html>